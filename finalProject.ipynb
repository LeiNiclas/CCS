{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCS - final project - Jona Maximilian Kao, Niclas Leinen\n",
    "This Jupyter Notebook contains all code for our bi-modal probabilistic models, including their implementation, training procedures and output analyses. \\\n",
    "To run the code, make sure all of the needed libraries are installed.\\\n",
    "<i>Note</i>: Executing all the code at once might take some time to compute. Also, the resulting values might deviate from the ones in our final report, as for some models there always is some random aspect in the process of minimizing the values. \\\n",
    "(Please also note that, although we mention the Log-Normal model to be our second model in the report, the implementation of the Log-Normal model can be found <i>after</i> the implementation LBA model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.stats import exponnorm\n",
    "from scipy.stats import ks_1samp\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Universal settings\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains some settings like setting the seed, paths and controlling whether to save generate figures or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "exponnorm.random_state = 42\n",
    "\n",
    "SAVE_FIGURES = True\n",
    "\n",
    "PATH_TO_MAT_FILES = \"Data/\"\n",
    "PATH_TO_PLOTS_REACTION_TIME = \"Plots/Reaction_times/\"\n",
    "PATH_TO_PLOTS_ACCURACIES = \"Plots/Accuracies/\"\n",
    "PATH_TO_PLOTS_MODEL_1 = \"Plots/Model_1/\"\n",
    "PATH_TO_PLOTS_MODEL_2 = \"Plots/Model_2/\"\n",
    "PATH_TO_PLOTS_MODEL_3 = \"Plots/Model_3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data import\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the path to the folder containing the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files = [f for f in os.listdir(PATH_TO_MAT_FILES) if f.endswith(\".mat\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a function that loads and extracts the contents of a .mat-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_file(file_path):\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "    # Extract data and convert to DataFrames\n",
    "    data_train = pd.DataFrame(mat_data['data_train'])\n",
    "    data_test = pd.DataFrame(mat_data['data_test'])\n",
    "    data_labels = [label[0] for label in mat_data['data_labels'][0]]  # Convert to list of column names\n",
    "\n",
    "    # Assign column names\n",
    "    data_train.columns = data_labels\n",
    "    data_test.columns = data_labels\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load all the data into two dictionaries: <i>TRAIN_DATA</i> and <i>TEST_DATA</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = {}\n",
    "TEST_DATA = {}\n",
    "\n",
    "for mat_file in mat_files:\n",
    "    filepath = os.path.join(PATH_TO_MAT_FILES, mat_file)\n",
    "    train, test = load_mat_file(filepath)\n",
    "    \n",
    "    # Store using participant ID (file name without .mat)\n",
    "    participant_id = mat_file.replace(\".mat\", \"\")\n",
    "    TRAIN_DATA[participant_id] = train\n",
    "    TEST_DATA[participant_id] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter the data with a missing action value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_filtered_actions = {}\n",
    "test_data_with_filtered_actions = {}\n",
    "\n",
    "for participant_id, data in TRAIN_DATA.items():\n",
    "    # Filter out rows where the action column has a value of 0\n",
    "    filtered_data = data[data[\"action (1=immediate, 2=delayed, 0=missing)\"] != 0]\n",
    "    filtered_data = filtered_data.reset_index(drop=True)\n",
    "    \n",
    "    train_data_with_filtered_actions[participant_id] = filtered_data\n",
    "\n",
    "for participant_id, data in TEST_DATA.items():\n",
    "    # Filter out rows where the action column has a value of 0\n",
    "    filtered_data = data[data[\"action (1=immediate, 2=delayed, 0=missing)\"] != 0]\n",
    "    filtered_data = filtered_data.reset_index(drop=True)\n",
    "    \n",
    "    test_data_with_filtered_actions[participant_id] = filtered_data\n",
    "\n",
    "# Replace the original dictionaries with the filtered ones\n",
    "TRAIN_DATA = train_data_with_filtered_actions\n",
    "TEST_DATA = test_data_with_filtered_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then define the labels of the data table for easier future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_IMM_OUT = \"immOutcome\"\n",
    "LABEL_DEL_OUT = \"delOutcome\"\n",
    "LABEL_DELAY = \"delay\"\n",
    "LABEL_CHOICE = \"action (1=immediate, 2=delayed, 0=missing)\"\n",
    "LABEL_P_IMM = \"p_imm\"\n",
    "LABEL_RT = \"RT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now fully loaded into <i>TRAIN_DATA</i> and <i>TEST_DATA</i> with the following layout: \\\n",
    "<i>data_set</i>[x] = data from participant x.\n",
    "\n",
    "We will be using these datasets for all of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a general idea of how the reaction times are distributed among participants, we can create some plots to analyze their behaviour. \\\n",
    "We first need to extract the reaction times from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reaction_times(data_set):\n",
    "    reaction_times = []\n",
    "    \n",
    "    # Get reaction times from each participant in the training data\n",
    "    for participant in data_set:\n",
    "        data_matrix = TRAIN_DATA[participant]\n",
    "        reaction_times.append((data_matrix[LABEL_RT]))\n",
    "    \n",
    "    return reaction_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can make a plot of the overall reaction time distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reaction_times = get_reaction_times(TRAIN_DATA)\n",
    "\n",
    "all_reaction_times_flattened = []\n",
    "\n",
    "for i in range(len(all_reaction_times)):\n",
    "    for rt in all_reaction_times[i]:\n",
    "        all_reaction_times_flattened.append(rt)\n",
    "    \n",
    "\n",
    "plt.hist(all_reaction_times_flattened, bins=30, range=(0, 8000), color=\"orange\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Reaction time (ms)\")\n",
    "plt.ylabel(\"Occurances\")\n",
    "plt.title(\"Overall reaction time distribution (Train data)\")\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_REACTION_TIME, \"overall_reaction_times\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another data point that might be interesting to look at is the correlation between reaction times and the choices made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reaction_times_immediate_outcome = []\n",
    "all_reaction_times_delayed_outcome = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    rts = TRAIN_DATA[participant][LABEL_RT]\n",
    "    choices = TRAIN_DATA[participant][LABEL_CHOICE]\n",
    "    \n",
    "    for j in range(len(rts)):\n",
    "        if choices[j] == 0:\n",
    "            continue\n",
    "        elif choices[j] == 1:\n",
    "            all_reaction_times_immediate_outcome.append(rts[j])\n",
    "        else:\n",
    "            all_reaction_times_delayed_outcome.append(rts[j])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "counts_immediate, bins_immediate = np.histogram(all_reaction_times_immediate_outcome, bins=35, range=(0, 8000))\n",
    "counts_delayed, bins_delayed = np.histogram(all_reaction_times_delayed_outcome, bins=35, range=(0, 8000))\n",
    "\n",
    "ax.bar(bins_delayed[:-1], counts_immediate, width=np.diff(bins_immediate), alpha=1, edgecolor=\"black\", label=\"Immediate outcome\")\n",
    "ax.bar(bins_delayed[:-1], -counts_delayed, width=np.diff(bins_delayed), color=\"red\", alpha=1, edgecolor=\"black\", label=\"Delayed outcome\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Reaction Time (ms)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(\"Reaction times for immediate vs. delayed outcomes\")\n",
    "\n",
    "max_count = max(max(counts_immediate), max(counts_delayed))\n",
    "ax.set_ylim(-max_count - 100, max_count + 100)\n",
    "ax.set_yticklabels([abs(int(tick)) for tick in ax.get_yticks()])\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_REACTION_TIME, \"reaction_times_delayed_vs_immediate\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hyperbolic Discounting <a id=\"hyperbolic-discounting\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following formulas in order to calculate our values for $k$ and $\\beta$:\n",
    "\n",
    "(1) $SV_{delayed,i}$ = $\\frac{delayed\\_outcome}{1+k*delay}$\n",
    "\n",
    "(2) $p_{delayed,i}$ = $\\frac{1}{1+e^{-\\beta(sv_{delayed} - sv_{immediate})}}$\n",
    "\n",
    "(3) $log_L$ = $\\sum_{i = 1}^n c_i log(p_{delayed,i}) + (1 - c_i) log(1-p_{delayed,i})$\n",
    "\n",
    "Therefore, we will need to make some intital estimates for $k$ and $\\beta$.\n",
    "In this case, we chose random values using the random library, and we got the values $k = 0.000237$ and $\\beta = 4.423591$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_k = 0.000237\n",
    "INITIAL_beta = 4.423591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the behaviour of the model when dealing with sample entries that are missing an <i>action</i>-value. (<i>action</i> = 0).\n",
    "<ul>\n",
    "    <li>Option 1: Set value to an <b>immediate</b> response.</li>\n",
    "    <li>Option 2: Set value to a <b>delayed</b> response.</li>\n",
    "    <li>Option 3: Set value to a <b>random</b> response.</li>\n",
    "    <li>Option 4: <b>Ignore</b> entries with missing <i>action</i>-value</li>\n",
    "</ul>\n",
    "\n",
    "For our very first attempts at building the models we used option 1 but we decided to switch to option 4 in the end, as we thought said values might contaminate the final results of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clamp action values between 0 and 1.\n",
    "# Remaps action values of 2.0 to 1.0 (delayed) and all other values to 0.0 (immediate).\n",
    "# This also covers the case of missing action values, which are treated as immediate.\n",
    "def action_values_clamp01(actions):\n",
    "    clamped_actions = []\n",
    "    \n",
    "    for action in actions:\n",
    "        if action == 2.0:\n",
    "            clamped_actions.append(1.0)\n",
    "        else:\n",
    "            clamped_actions.append(0.0)\n",
    "    \n",
    "    return clamped_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $k$-log-likelihood-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_k(k, data_set, participant):\n",
    "    beta = INITIAL_beta\n",
    "    log_L = 0\n",
    "\n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    \n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    \n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        sv_delayed = delayed_outcomes[i] / (1.0 + k * max(delays[i], 1e-10))\n",
    "        \n",
    "        p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[i])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        log_L += clamped_actions[i] * np.log(p_delayed) + (1.0 - clamped_actions[i]) * np.log(1.0 - p_delayed)\n",
    "\n",
    "    return -log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize $k$ values with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_k = []\n",
    "\n",
    "for participant in TRAIN_DATA:\n",
    "    result = minimize(log_likelihood_k, INITIAL_k, args=(TRAIN_DATA, participant))\n",
    "    results_k.append(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\beta$-log-likelihood-function using calculated $k$-values <a id=\"m1_beta_llf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_beta(beta, data_set, participant, participant_id):\n",
    "    # Retrieve the k value for the current participant\n",
    "    k = results_k[participant_id]\n",
    "    log_L = 0\n",
    "\n",
    "    # Get the data matrix for the current participant\n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    # Determine the sample size\n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    # Extract relevant columns from the data matrix\n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    \n",
    "    # Clamp action values to be between 0 and 1\n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "\n",
    "    # Loop through each sample in the data matrix\n",
    "    for i in range(sample_size):\n",
    "        # Calculate the subjective value of the delayed outcome\n",
    "        sv_delayed = delayed_outcomes[i] / (1.0 + k * max(delays[i], 1e-10))\n",
    "        \n",
    "        # Calculate the probability of choosing the delayed outcome\n",
    "        p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[i])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        # Update the log likelihood\n",
    "        log_L += clamped_actions[i] * np.log(p_delayed) + (1.0 - clamped_actions[i]) * np.log(1.0 - p_delayed)\n",
    "\n",
    "\n",
    "    return -log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize $\\beta$ with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store beta results\n",
    "results_beta = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    # Minimize the log likelihood function to find the best beta value\n",
    "    result = minimize(log_likelihood_beta, INITIAL_beta, args=(TRAIN_DATA, participant, i))\n",
    "    results_beta.append(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have $k$ and $\\beta$ values that we can universally use for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that predicts choices given the $k$ and $\\beta$ of the participant. \\\n",
    "The model will predict the action to be <i>immediate</i> if $p \\leq 0.5$, else <i>delayed</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_choices(k, beta, participant):\n",
    "    data_matrix = TEST_DATA[participant]\n",
    "\n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "\n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    predicted_choices = []\n",
    "    \n",
    "    # Loop through each sample in the data matrix\n",
    "    for i in range(sample_size):\n",
    "        # Calculate the subjective value of the delayed outcome\n",
    "        sv_delayed = delayed_outcomes[i] / (1.0 + k * max(delays[i], 1e-10))\n",
    "        \n",
    "        # Calculate the probability of choosing the delayed outcome\n",
    "        p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[i])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        if p_delayed > 0.5:\n",
    "            predicted_choices.append(1)\n",
    "        else:\n",
    "            predicted_choices.append(0)\n",
    "    \n",
    "    return predicted_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further define a function that calculates the prediction accuries by comparing the predictions to the actual choices made in the test-dataset. The output ranges from 0-100 (percentage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracies(k, beta, participant):\n",
    "    predicted_choices = predict_choices(k, beta, participant)\n",
    "    \n",
    "    actions = TEST_DATA[participant][LABEL_CHOICE]\n",
    "    \n",
    "    sample_size = len(actions)\n",
    "    \n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "\n",
    "    correct_guesses = 0\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        if clamped_actions[i] == predicted_choices[i]:\n",
    "            correct_guesses += 1\n",
    "    \n",
    "    return correct_guesses / sample_size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the accuracies we would get when using the $k$ and $\\beta$ values we calculated above only (meaning no model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_ACCURACIES_k_beta = []\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    accuracy_score = predict_accuracies(results_k[i], results_beta[i], participant)\n",
    "    PREDICTION_ACCURACIES_k_beta.append(accuracy_score)\n",
    "    \n",
    "print(f\"Average Accuracy: {np.mean(PREDICTION_ACCURACIES_k_beta):.2f}%\")\n",
    "\n",
    "plt.hist(PREDICTION_ACCURACIES_k_beta, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Accuracies for each participant (Hyperbolic discounting)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_ACCURACIES, \"accuracy_hyperbolic_discounting\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get started with our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model 1: Ex-Gaussian\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit an Ex-Gaussian distribution to the reaction time data using scipy.exponnorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RTs = get_reaction_times(TRAIN_DATA)\n",
    "\n",
    "ex_gaussian_fits = []\n",
    "\n",
    "for rt in TRAIN_RTs:\n",
    "    ex_gaussian_fits.append(exponnorm.fit(rt))\n",
    "\n",
    "n = len(ex_gaussian_fits)\n",
    "\n",
    "# Extracting the parameters\n",
    "K_values = [ex_gaussian_fits[i][0] for i in range(n)]\n",
    "loc_values = [ex_gaussian_fits[i][1] for i in range(n)]\n",
    "scale_values = [ex_gaussian_fits[i][2] for i in range(n)]\n",
    "\n",
    "tau_values = [K_values[i] * scale_values[i] for i in range(n)]\n",
    "mu_values = loc_values\n",
    "sigma_values = scale_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store fitted CDFs, KS statistics, and p-values\n",
    "fitted_cdfs = []\n",
    "ks_stats = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(len(K_values)):\n",
    "    # Create a lambda function for the fitted CDF using the exponentially modified normal distribution\n",
    "    fitted_cdfs.append(lambda x: exponnorm.cdf(x, K_values[i], loc=mu_values[i], scale=sigma_values[i]))\n",
    "    \n",
    "    ks_stat, p_value = ks_1samp(TRAIN_RTs[i], fitted_cdfs[i])\n",
    "    \n",
    "    ks_stats.append(ks_stat)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for p_value in p_values:\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Reject null hypothesis for participant with id {i}: {p_value:.4f}\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(np.mean(p_values))\n",
    "print(min(p_values))\n",
    "\n",
    "# [DEBUG] Print the mean of all p-values\n",
    "# print(f\"Mean of all p_values: {np.mean(p_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hyptothesis test demonstrates that the model holds true for all participants except for participant <b>#57</b>. \\\n",
    "The mean <i>p_value</i> lies at 0.7738. \\\n",
    "For every single other participant, the <i>p_value</i> is much greater than 0.05, meaning that there is an extremely small to no difference between our data and the ex-gaussian distribution. Therefore, we are going to use this approach to build our bimodal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a unified log-likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the Ex-Gaussian-likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_gaussian_likelihood(participant_reaction_times, K_value, mu_value, sigma_value):\n",
    "    return exponnorm.logpdf(participant_reaction_times, K_value, loc=mu_value, scale=sigma_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint-log-likelihood can be calculated by the expression  $$log_{L,i}(joint)=log_{L,i}(p_{delay, i}) + log_{L,i}(reactiontime_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_likelihood_train(params, data_set, participant, i):\n",
    "    log_L = 0\n",
    "    mu, sigma, K, beta, k = params\n",
    "    \n",
    "    # Initialize the log likelihood for the current participant\n",
    "    Log_L_p_delay = 0\n",
    "    log_L_rt = 0\n",
    "    \n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    \n",
    "    # Clamp action values to be between 0 and 1\n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        reaction_time_likelihood = ex_gaussian_likelihood(TRAIN_RTs[i][j], K, mu, sigma)\n",
    "\n",
    "        log_L_rt += reaction_time_likelihood\n",
    "        \n",
    "        # Calculate the subjective value of the delayed outcome\n",
    "        sv_delayed = delayed_outcomes[j] / (1.0 + k * max(delays[j], 1e-10))\n",
    "        \n",
    "        # Calculate the probability of choosing the delayed outcome\n",
    "        p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[j])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        # Update the log likelihood for the current participant\n",
    "        Log_L_p_delay += clamped_actions[j] * np.log(p_delayed) + (1.0 - clamped_actions[j]) * np.log(1.0 - p_delayed)\n",
    "    \n",
    "    # Update the overall log likelihood\n",
    "    log_L += Log_L_p_delay\n",
    "    log_L += log_L_rt\n",
    "    \n",
    "    return -log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we can now optimize $\\mu$, $\\sigma$, $K$, $\\beta$ and $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize index and results list\n",
    "results_model_1 = []\n",
    "\n",
    "# Loop through each participant in the training data\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    \n",
    "    # Extract the initial parameters for the current participant\n",
    "    # initial params seemed buggy, potential fix:\n",
    "    INITIAL_PARAMS_MODEL_1 = [\n",
    "        np.ravel(mu_values[i])[0],\n",
    "        np.ravel(sigma_values[i])[0],\n",
    "        np.ravel(K_values[i])[0],\n",
    "        np.ravel(results_beta[i])[0],\n",
    "        np.ravel(results_k[i])[0]\n",
    "    ]\n",
    "    \n",
    "    BOUNDS_MODEL_1 = [\n",
    "        (None, None),\n",
    "        (None, None),\n",
    "        (None, None),\n",
    "        (0.1, 10),\n",
    "        (1e-6, 1)\n",
    "    ]\n",
    "    \n",
    "    # Convert the initial parameters to np.array\n",
    "    initial_params = np.array(INITIAL_PARAMS_MODEL_1)\n",
    "    \n",
    "    # Minimizing\n",
    "    result = minimize(joint_log_likelihood_train, INITIAL_PARAMS_MODEL_1, args=(TRAIN_DATA, participant, i), bounds=BOUNDS_MODEL_1)\n",
    "    results_model_1.append(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing Model 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC & BIC test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that computes the AIC and BIC values and returns them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic_bic(log_likelihood, num_params, num_trials):\n",
    "    AIC = 2 * num_params - 2 * log_likelihood\n",
    "    BIC = num_params * np.log(num_trials) - 2 * log_likelihood\n",
    "    \n",
    "    return AIC, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aics_model_1 = []\n",
    "bics_model_1 = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    params = results_model_1[i]\n",
    "    \n",
    "    joint_log_likelihood_model_1 = joint_log_likelihood_train(params, TRAIN_DATA, participant, i)\n",
    "    num_trials = len(TRAIN_DATA[participant])\n",
    "    \n",
    "    aic, bic = compute_aic_bic(-joint_log_likelihood_model_1, len(params), num_trials)\n",
    "    \n",
    "    aics_model_1.append(aic)\n",
    "    bics_model_1.append(bic)\n",
    "\n",
    "print(f\"AIC: {np.mean(aics_model_1):.2f}\")\n",
    "print(f\"BIC: {np.mean(bics_model_1):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS and MSE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RTs = get_reaction_times(TEST_DATA)\n",
    "\n",
    "p_values_model_1 = []\n",
    "mse_values_test_model_1 = []\n",
    "\n",
    "output_dir = \"Plots/Model_1/QQ/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    predicted_reaction_times = exponnorm.rvs(results_model_1[i][2], results_model_1[i][0], results_model_1[i][1], size=len(TEST_DATA[participant]))\n",
    "    \n",
    "    ks_stat, p_value = ks_2samp(predicted_reaction_times, TEST_RTs[i])\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"Reject null hypothesis for participant with id {i}: {p_value:.4f}\")\n",
    "    \n",
    "    p_values_model_1.append(p_value)\n",
    "    \n",
    "    sample_size = len(TEST_RTs[i])\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        error_seconds = ((TEST_DATA[participant][\"RT\"][j] - predicted_reaction_times[j])/1000)**2\n",
    "        \n",
    "        mse += error_seconds\n",
    "        \n",
    "    mse /= sample_size\n",
    "    \n",
    "    mse_values_test_model_1.append(mse)\n",
    "\n",
    "    stats.probplot(TEST_RTs[i], dist=exponnorm, sparams=(results_model_1[i][2], results_model_1[i][0], results_model_1[i][1]), plot=plt)\n",
    "    plt.title(f\"Q-Q Plot: Observed RTs vs. Predicted RTs (Ex-Gaussian, Participant {i})\")\n",
    "    \n",
    "    plt.legend([\"Observed RTs\", \"Predicted RTs\"])\n",
    "    \n",
    "    if SAVE_FIGURES:\n",
    "        plot_filename = os.path.join(output_dir, f\"qq_plot_participant_{i}.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(f\"Average p-value: {np.mean(p_values_model_1)}\")\n",
    "print(f\"Minimum p-value: {np.min(p_values_model_1)}\")\n",
    "print(f\"Average MSE value: {np.mean(mse_values_test_model_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test overall accuracy, we will define a function that simulates always picking <i>the same option</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_choice_accuracy(participant, option):\n",
    "    correct_guesses = 0\n",
    "    \n",
    "    actions = TEST_DATA[participant][LABEL_CHOICE]\n",
    "    \n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    for i in range(len(clamped_actions)):\n",
    "        if clamped_actions[i] == option:\n",
    "            correct_guesses += 1\n",
    "    \n",
    "    return correct_guesses / len(clamped_actions) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute these fixed-choice accuracies <b>once</b> and reuse them for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_accuracies_model_1 = []\n",
    "\n",
    "ACCURACY_SCORES_FIXED_CHOICE_IMMEDIATE = []\n",
    "ACCURACY_SCORES_FIXED_CHOICE_DELAYED = []\n",
    "\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    accuracy_score = predict_accuracies(results_model_1[i][4], results_model_1[i][3], participant)\n",
    "    prediction_accuracies_model_1.append(accuracy_score)\n",
    "\n",
    "    # These are the values that are only computed once.\n",
    "    # They don't change per model.\n",
    "    accuracy_score_always_immediate = fixed_choice_accuracy(participant, 0)\n",
    "    accuracy_score_always_delayed = 100 - accuracy_score_always_immediate\n",
    "    \n",
    "    ACCURACY_SCORES_FIXED_CHOICE_IMMEDIATE.append(accuracy_score_always_immediate)\n",
    "    ACCURACY_SCORES_FIXED_CHOICE_DELAYED.append(accuracy_score_always_delayed)\n",
    "\n",
    "print(f\"Our model precision: {np.mean(prediction_accuracies_model_1):.2f}%\")\n",
    "print(f\"Always choosing \\\"immediate\\\": {np.mean(ACCURACY_SCORES_FIXED_CHOICE_IMMEDIATE):.2f}%\")\n",
    "print(f\"Always choosing \\\"delayed\\\": {np.mean(ACCURACY_SCORES_FIXED_CHOICE_DELAYED):.2f}%\")\n",
    "\n",
    "plt.hist(PREDICTION_ACCURACIES_k_beta, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Accuracies for each participant (Hyperbolic discounting)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(prediction_accuracies_model_1, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Accuracies for each participant (Ex-Gaussian)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_ACCURACIES, \"accuracy_ex_gaussian\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model 2: LBA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our PDF-Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lba_pdf(rt, v, b, A, t0, choice):\n",
    "    rt_adj = rt - t0\n",
    "    \n",
    "    if rt_adj <= 0:\n",
    "        return 1e-6\n",
    "    \n",
    "    choice = int(choice)\n",
    "    v_win = v[choice]\n",
    "    v_lose = v[1 - choice]\n",
    "    \n",
    "    if v_win <= 0 or v_lose <= 0:\n",
    "        return 1e-6\n",
    "    \n",
    "    t = rt_adj\n",
    "    density = (v_win / t) * (1 - np.exp(-v_win * (b - A) / t)) * np.exp(-v_lose * (b - A) / t)\n",
    "    \n",
    "    if np.isfinite(density):\n",
    "        return max(density, 1e-6)\n",
    "    \n",
    "    return 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create our log likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBA Log-Likelihood (no debug prints)\n",
    "def lba_log_likelihood(params, data_set, participant):\n",
    "    v_imm, v_del, b, A, t0 = params\n",
    "    v = [v_imm, v_del]\n",
    "    \n",
    "    log_likelihood = 0\n",
    "    \n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    \n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        rt = data_matrix[\"RT\"][j] / 1000\n",
    "        choice = clamped_actions[j]\n",
    "        \n",
    "        density = lba_pdf(rt, v, b, A, t0, choice)\n",
    "        log_likelihood += np.log(max(density, 1e-6))\n",
    "    \n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the joint likelihood function for the LBA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_likelihood_model_2(params, data_set, participant):\n",
    "    log_L = 0\n",
    "    v1, v2, b, A, t0, k, beta = params\n",
    "    \n",
    "    # Initialize the log likelihood for the current participant\n",
    "    Log_L_p_delay = 0\n",
    "    \n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    sample_size = len(data_matrix)\n",
    "    \n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    \n",
    "    # Clamp action values to be between 0 and 1\n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        # Calculate the subjective value of the delayed outcome\n",
    "        sv_delayed = delayed_outcomes[j] / (1.0 + k * max(delays[j], 1e-10))\n",
    "        \n",
    "        p_delayed = 1.0 / np.clip(1.0 + np.exp(-beta * (sv_delayed - sv_immediates[j])), 1, 10000)\n",
    "        \n",
    "        \n",
    "        # Calculate the probability of choosing the delayed outcome\n",
    "        # p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[j])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        # Update the log likelihood for the current participant\n",
    "        Log_L_p_delay += clamped_actions[j] * np.log(p_delayed) + (1.0 - clamped_actions[j]) * np.log(1.0 - p_delayed)\n",
    "    \n",
    "    log_L_lba = lba_log_likelihood([v1, v2, b, A, t0], data_set, participant)\n",
    "    \n",
    "    # Update the overall log likelihood\n",
    "    log_L += Log_L_p_delay\n",
    "    log_L += log_L_lba\n",
    "    \n",
    "    return -log_L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the bounds for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUNDS_MODEL_2 = [\n",
    "    (0.1, 1),  # v1 [0]\n",
    "    (0.1, 1),  # v2 [1]\n",
    "    (0.1, 5),  # b [2]\n",
    "    (0.1, 5),  # A [3]\n",
    "    (0.1, 0.5),  # t0 [4]\n",
    "    (1e-6, 1),  # k [5]\n",
    "    (0.1, 10)   # beta [6]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model_2 = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    INITIAL_PARAMS_MODEL_2 = [ 0.5, 0.5, 1.5, 1.5, 0.3, INITIAL_k, INITIAL_beta ]\n",
    "    \n",
    "    # The combination of differential_evolution results as the initial parameters\n",
    "    # to the minimize function yields the best values for the models prediction and accuracy scores.\n",
    "    result = differential_evolution(joint_log_likelihood_model_2, BOUNDS_MODEL_2, args=(TRAIN_DATA, participant))\n",
    "    refined_result = minimize(joint_log_likelihood_model_2, result.x, args=(TRAIN_DATA, participant), bounds=BOUNDS_MODEL_2, method=\"Nelder-Mead\")\n",
    "    results_model_2.append(refined_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DEBUG] show all results\n",
    "# for result in results_model_2:\n",
    "#    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing Model 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC & BIC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aics_model_2 = []\n",
    "bics_model_2 = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    params = results_model_2[i]\n",
    "    \n",
    "    likelihood = joint_log_likelihood_model_2(params, TRAIN_DATA, participant)\n",
    "    num_trials = len(TRAIN_DATA[participant])\n",
    "    \n",
    "    aic, bic = compute_aic_bic(likelihood, len(params), num_trials)\n",
    "    \n",
    "    aics_model_2.append(aic)\n",
    "    bics_model_2.append(bic)\n",
    "\n",
    "print(f\"AIC: {np.mean(aics_model_2):.2f}\")\n",
    "print(f\"BIC: {np.mean(bics_model_2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_accuracies_model_2 = []\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    accuracy_score = predict_accuracies(results_model_2[i][5], results_model_2[i][6], participant)\n",
    "    prediction_accuracies_model_2.append(accuracy_score)\n",
    "\n",
    "print(f\"Accuracy: {np.mean(prediction_accuracies_model_2):.2f}%\")\n",
    "\n",
    "plt.hist(PREDICTION_ACCURACIES_k_beta, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Accuracies for each participant (Hyperbolic discounting)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(prediction_accuracies_model_2, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Accuracies for each participant (LBA)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_ACCURACIES, \"accuracy_lba\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS and MSE test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have a function that simulates the reaction times based on the results of $v_1$, $v_2$, $A$, $b$ and $t_0$ produced by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lba_reaction_times(v1, v2, A, b, t0, size):\n",
    "    # Sample drift rates from a uniform distribution (can be modified)\n",
    "    drift_rates = np.column_stack([\n",
    "        np.random.uniform(v1 * 0.9, v1 * 1.1, size=size),  # Jitter around v1\n",
    "        np.random.uniform(v2 * 0.9, v2 * 1.1, size=size)   # Jitter around v2\n",
    "    ])\n",
    "    \n",
    "    # Sample starting points from a uniform distribution between 0 and A\n",
    "    start_points = np.random.uniform(0, A, size=(size, 2))\n",
    "    \n",
    "    # Compute time to threshold for each accumulator\n",
    "    thresholds = b - start_points\n",
    "    \n",
    "    decision_times = thresholds / drift_rates  # Time to reach the boundary\n",
    "    \n",
    "    # Choose the winning accumulator (minimum RT wins)\n",
    "    winning_indices = np.argmin(decision_times, axis=1)  # 0 or 1\n",
    "    \n",
    "    # Get reaction times by selecting the min decision time per trial\n",
    "    reaction_times = np.min(decision_times, axis=1) - t0\n",
    "\n",
    "    return reaction_times, winning_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "output_dir = \"Plots/Model_2/QQ/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "TEST_RTs = get_reaction_times(TEST_DATA)\n",
    "\n",
    "p_values_model_2 = []\n",
    "mse_values_test_model_2 = []\n",
    "\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    v1, v2, A, b, t0, k, beta = results_model_2[i]\n",
    "    \n",
    "    predicted_reaction_times, _ = simulate_lba_reaction_times(v1, v2, A, b, t0, size=len(TEST_DATA[participant]))\n",
    "\n",
    "    # Adjust the prediction reaction times (seconds) to milliseconds\n",
    "    ks_stat, p_value = ks_2samp(predicted_reaction_times, TEST_RTs[i] / 1000)\n",
    "    \n",
    "    p_values_model_2.append(p_value)\n",
    "    \n",
    "    sample_size = len(TEST_RTs[i])\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        error_seconds = ((TEST_DATA[participant][\"RT\"][j]/1000 - predicted_reaction_times[j]))**2\n",
    "        \n",
    "        mse += error_seconds\n",
    "        \n",
    "    mse /= sample_size\n",
    "    \n",
    "    mse_values_test_model_2.append(mse)\n",
    "    \n",
    "    # Generate the Q-Q plot\n",
    "    plt.figure()\n",
    "    stats.probplot(TEST_RTs[i], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q Plot: Observed RTs vs. Predicted RTs (LBA, Participant {i})\")\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(['Observed RTs', 'Predicted RTs'])\n",
    "\n",
    "    # Save the plot to the specified folder\n",
    "    if SAVE_FIGURES:\n",
    "        plot_filename = os.path.join(output_dir, f\"qq_plot_participant_{i}.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "    \n",
    "    plt.show()  # Close the plot to avoid displaying it in the notebook\n",
    "\n",
    "print(np.mean(p_values_model_2))\n",
    "print(np.min(p_values_model_2))\n",
    "print(np.mean(mse_values_test_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values are not really great, this can only mean one of two things:\n",
    "<ol>\n",
    "    <li>Our approach to <b>train</b> our model is wrong.</li>\n",
    "    <li>Our function to <b>generate predictions</b> with our evaluations is wrong.</li>\n",
    "</ol>\n",
    "\n",
    "Sadly, we could not find a fix to this. We think it might be the latter option, as we can see that the accuracy values are not too bad and in line with the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model 3: Log-Normal\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the joint log likelihood function for our model. We will be using one of the formulas presented in the lectures:\n",
    "\n",
    "$$ log_L = \\sum_{i = 0}^N \\log_{L,i,p_{delay}} + \\ell_{L,i} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\ell_{L,i} = \\sum_{j = 0}^N -\\log(RT_j) - \\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(\\log(RT_j) - \\mu)^2}{2\\sigma^2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_likelihood_model_3(params, data_set, participant):\n",
    "    log_L = 0\n",
    "    k, beta, mu, sigma = params\n",
    "\n",
    "    Log_L_p_delay = 0\n",
    "    \n",
    "    # Get the data matrix for the current participant\n",
    "    data_matrix = data_set[participant]\n",
    "    \n",
    "    sample_size = len(data_matrix)\n",
    "   \n",
    "    # Extract relevant columns from the data matrix\n",
    "    sv_immediates = data_matrix[LABEL_IMM_OUT]\n",
    "    delayed_outcomes = data_matrix[LABEL_DEL_OUT]\n",
    "    delays = data_matrix[LABEL_DELAY]\n",
    "    actions = data_matrix[LABEL_CHOICE]\n",
    "    reaction_times = data_matrix[LABEL_RT]\n",
    "    \n",
    "    # Clamp action values to be between 0 and 1\n",
    "    clamped_actions = action_values_clamp01(actions)\n",
    "    \n",
    "    log_L_model_3 = 0\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        log_L_model_3 += - np.log(reaction_times[j]) - 0.5 * np.log(2 * np.pi * (sigma**2)) - ((np.log(reaction_times[j]) - mu)**2) / (2 * (sigma**2))\n",
    "        \n",
    "        # Calculate the subjective value of the delayed outcome\n",
    "        sv_delayed = delayed_outcomes[j] / (1.0 + k * max(delays[j], 1e-10))\n",
    "        \n",
    "        # Calculate the probability of choosing the delayed outcome\n",
    "        p_delayed = 1.0 / (1.0 + np.exp(-beta * (sv_delayed - sv_immediates[j])))\n",
    "        p_delayed = np.clip(p_delayed, 1e-10, 1.0 - 1e-10)\n",
    "        \n",
    "        # Update the log likelihood for the current participant\n",
    "        Log_L_p_delay += clamped_actions[j] * np.log(p_delayed) + (1.0 - clamped_actions[j]) * np.log(1.0 - p_delayed)\n",
    "    \n",
    "    # Update the overall log likelihood\n",
    "    log_L += Log_L_p_delay\n",
    "    log_L += log_L_model_3\n",
    "    \n",
    "    return -log_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model_3 = []\n",
    "\n",
    "log_mean = 0\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    BOUNDS_MODEL_3 = [\n",
    "        (1e-6, 0.1),    # bounds for k\n",
    "        (0.1, 20),      # bounds for beta\n",
    "        (6, 9),         # bounds for mu\n",
    "        (1e-2, 1)       # bounds for sigma\n",
    "    ]\n",
    "\n",
    "    result = differential_evolution(joint_log_likelihood_model_3, BOUNDS_MODEL_3, args=(TRAIN_DATA, participant))\n",
    "    results_model_3.append(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DEBUG] printing the results\n",
    "# for result in results_model_3:\n",
    "#    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing Model 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC & BIC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aics_model_3 = []\n",
    "bics_model_3 = []\n",
    "\n",
    "for i, participant in enumerate(TRAIN_DATA):\n",
    "    params = results_model_3[i]\n",
    "    \n",
    "    likelihood = joint_log_likelihood_model_3(params, TRAIN_DATA, participant)\n",
    "    num_trials = len(TRAIN_DATA[participant])\n",
    "    \n",
    "    aic, bic = compute_aic_bic(-likelihood, len(params), num_trials)\n",
    "    \n",
    "    aics_model_3.append(aic)\n",
    "    bics_model_3.append(bic)\n",
    "\n",
    "print(f\"AIC: {np.mean(aics_model_3):.2f}\")\n",
    "print(f\"BIC: {np.mean(bics_model_3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS & MSE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values_test_model_3 = []\n",
    "p_values_model_3 = []\n",
    "\n",
    "rejections = 0\n",
    "\n",
    "output_dir = os.path.join(PATH_TO_PLOTS_MODEL_3, \"QQ/\")\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    mu = results_model_3[i][2]\n",
    "    sigma = results_model_3[i][3]\n",
    "    \n",
    "    sample_size = len(TEST_RTs[i])\n",
    "    \n",
    "    predicted_reaction_times = np.random.lognormal(mean=mu, sigma=sigma, size=sample_size)\n",
    "\n",
    "    ks_stat, p_value = ks_2samp(predicted_reaction_times, TEST_RTs[i])\n",
    "    \n",
    "    p_values_model_3.append(p_value)\n",
    "    \n",
    "    stats.probplot(TEST_RTs[i], dist=\"lognorm\", sparams=(sigma, 0, np.exp(mu)), plot=plt)\n",
    "    plt.title(f\"Q-Q Plot: Observed RTs vs. Predicted RTs (Log-Normal, Participant {i})\")\n",
    "    \n",
    "    plt.legend([\"Observed RTs\", \"Predicted RTs\"])\n",
    "    \n",
    "    if SAVE_FIGURES:\n",
    "        plot_filename = os.path.join(output_dir, f\"qq_plot_participant_{i}.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for j in range(sample_size):\n",
    "        error_seconds = ((TEST_DATA[participant][\"RT\"][j] - predicted_reaction_times[j])/1000)**2\n",
    "        \n",
    "        mse += error_seconds\n",
    "        \n",
    "    mse /= sample_size\n",
    "    \n",
    "    mse_values_test_model_3.append(mse)\n",
    "    \n",
    "    if p_value < 0.5:\n",
    "        rejections += 1\n",
    "\n",
    "print(f\"mean p-value: {np.mean(p_values_model_3):.2f}\")\n",
    "print(f\"min p-value: {np.min(p_values_model_3):.2f}\")\n",
    "print(f\"MSE: {np.mean(mse_values_test_model_3):.2f}\")\n",
    "print(f\"Rejections: {rejections}\")\n",
    "\n",
    "plt.hist(mse_values_test_model_3, bins=10, range=(0, 3), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_accuracies_model_3 = []\n",
    "\n",
    "for i, participant in enumerate(TEST_DATA):\n",
    "    accuracy_score = predict_accuracies(results_model_3[i][0], results_model_3[i][1], participant)\n",
    "    \n",
    "    accuracy_score_always_immediate = fixed_choice_accuracy(participant, 0)\n",
    "    accuracy_score_always_delayed = 100 - accuracy_score_always_immediate\n",
    "    \n",
    "    prediction_accuracies_model_3.append(accuracy_score)\n",
    "\n",
    "print(f\"Our model precision: {np.mean(prediction_accuracies_model_3):.2f}%\")\n",
    "print(f\"Always choosing \\\"immediate\\\": {np.mean(ACCURACY_SCORES_FIXED_CHOICE_IMMEDIATE):.2f}%\")\n",
    "print(f\"Always choosing \\\"delayed\\\": {np.mean(ACCURACY_SCORES_FIXED_CHOICE_DELAYED):.2f}%\")\n",
    "\n",
    "plt.hist(PREDICTION_ACCURACIES_k_beta, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Total accuracy distribution (Hyperbolic discounting)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(prediction_accuracies_model_3, bins=10, range=(0, 100), color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Total accuracy distribution (Log-Normal)\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plot_filename = os.path.join(PATH_TO_PLOTS_ACCURACIES, \"accuracy_log_normal\")\n",
    "    plt.savefig(plot_filename)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
